{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Exported ONNX Model in FINN\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** We recommend clicking **Cell -> Run All** when you start reading this notebook for \"latency hiding\".</font>\n",
    "\n",
    "**Important: This notebook depends on the 1-train-mlp-with-brevitas notebook, because we are using the ONNX model that was exported there. So please make sure the needed .onnx file is generated before you run this notebook.**\n",
    "\n",
    "**Also remember to 'close and halt' any other FINN notebooks, since Netron visualizations use the same port.**\n",
    "\n",
    "In this notebook we will show how to import the network we trained in Brevitas and verify it in the FINN compiler. \n",
    "This verification process can actually be done at various stages in the compiler [as explained in this notebook](../bnn-pynq/tfc_end2end_verification.ipynb) but for this example we'll only consider the first step: verifying the exported high-level FINN-ONNX model.\n",
    "Another goal of this notebook is to introduce you to the concept of *graph transformations* -- we'll be applying some transformations to the graph to make it executable for verification. \n",
    "Once this model is sucessfully verified, we'll generate an FPGA accelerator from it in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is important -- always import onnx before torch**. This is a workaround for a [known bug](https://github.com/onnx/onnx/issues/2394)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "1. [Import model into FINN with ModelWrapper](#brevitas_import_visualization)\n",
    "2. [Network preparations: Tidy-up transformations](#network_preparations)\n",
    "3. [Load the dataset and Brevitas model](#load_dataset) \n",
    "4. [Compare FINN and Brevitas execution](#compare_brevitas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import model into FINN with ModelWrapper <a id=\"brevitas_import_visualization\"></a>\n",
    "\n",
    "Now that we have the model in .onnx format, we can work with it using FINN. To import it into FINN, we'll use the [`ModelWrapper`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#qonnx.core.modelwrapper.ModelWrapper). It is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "base_dir = os.environ['FINN_ROOT'] + \"/notebooks/FINN_Brevitas/\"\n",
    "build_dir = base_dir + \"build/\"\n",
    "\n",
    "brevitas_model_filename = build_dir + \"brevitas-ready.onnx\"\n",
    "brevitas_model = ModelWrapper(brevitas_model_filename)\n",
    "\n",
    "finn_model_filename = build_dir + \"finn-ready.onnx\"\n",
    "#finn_model_filename = build_dir + \"end2end_cnv_w1a1_tidy.onnx\"\n",
    "#finn_model_filename = build_dir + \"end2end_cnv_w1a1_pre_post.onnx\"\n",
    "#finn_model_filename = build_dir + \"end2end_cnv_w1a1_streamlined.onnx\"\n",
    "#finn_model_filename = build_dir + \"end2end_cnv_w1a1_dataflow_parent.onnx\"\n",
    "\n",
    "#finn_model_filename = build_dir + \"output_ipstitch_ooc_rtlsim/intermediate_models/step_hw_codegen.onnx\"\n",
    "\n",
    "finn_model = ModelWrapper(finn_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the member functions exposed by `ModelWrapper` to see what kind of information we can extract from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_model_proto',\n",
       " 'analysis',\n",
       " 'check_all_tensor_shapes_specified',\n",
       " 'check_compatibility',\n",
       " 'cleanup',\n",
       " 'find_consumer',\n",
       " 'find_consumers',\n",
       " 'find_direct_predecessors',\n",
       " 'find_direct_successors',\n",
       " 'find_producer',\n",
       " 'find_upstream',\n",
       " 'fix_float64',\n",
       " 'get_all_tensor_names',\n",
       " 'get_finn_nodes',\n",
       " 'get_initializer',\n",
       " 'get_metadata_prop',\n",
       " 'get_node_from_name',\n",
       " 'get_node_index',\n",
       " 'get_nodes_by_op_type',\n",
       " 'get_non_finn_nodes',\n",
       " 'get_tensor_datatype',\n",
       " 'get_tensor_fanout',\n",
       " 'get_tensor_layout',\n",
       " 'get_tensor_shape',\n",
       " 'get_tensor_sparsity',\n",
       " 'get_tensor_valueinfo',\n",
       " 'graph',\n",
       " 'is_fork_node',\n",
       " 'is_join_node',\n",
       " 'make_empty_exec_context',\n",
       " 'make_new_valueinfo_name',\n",
       " 'model',\n",
       " 'rename_tensor',\n",
       " 'save',\n",
       " 'set_initializer',\n",
       " 'set_metadata_prop',\n",
       " 'set_tensor_datatype',\n",
       " 'set_tensor_layout',\n",
       " 'set_tensor_shape',\n",
       " 'set_tensor_sparsity',\n",
       " 'temporary_fix_oldstyle_domain',\n",
       " 'transform']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(finn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these helper functions relate to extracting information about the structure and properties of the ONNX model. You can find out more about examining and manipulating ONNX models programmatically in [this tutorial](../../basics/0_how_to_work_with_onnx.ipynb), but we'll show a few basic functions here. For instance, we can extract the shape and datatype annotation for various tensors in the graph, as well as information related to the operation types associated with each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "Input tensor shape: [1, 3, 224, 224]\n",
      "Output tensor shape: [1, 4]\n",
      "Input tensor datatype: FLOAT32\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Conv', 'Mul', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'BatchNormalization', 'MultiThreshold', 'Mul', 'Flatten', 'MatMul', 'Mul']\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_out_tensor_name = finn_model.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = finn_model.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = finn_model.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = finn_model.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in finn_model.graph.node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output tensor is (as of yet) marked as a float32 value, even though we know the output is binary. This will be automatically inferred by the compiler in the next step when we run the `InferDataTypes` transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network preparation: Tidy-up transformations <a id=\"network_preparations\"></a>\n",
    "\n",
    "Before running the verification, we need to prepare our FINN-ONNX model. In particular, all the intermediate tensors need to have statically defined shapes. To do this, we apply some graph transformations to the model like a kind of \"tidy-up\" to make it easier to process. \n",
    "\n",
    "**Graph transformations in FINN.** The whole FINN compiler is built around the idea of transformations, which gradually transform the model into a synthesizable hardware description. Although FINN offers functionality that automatically calls a standard sequence of transformations (covered in the next notebook), you can also manually call individual transformations (like we do here), as well as adding your own transformations, to create custom flows. You can read more about these transformations in [this notebook](../bnn-pynq/tfc_end2end_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "#finn_model = finn_model.transform(InferShapes())\n",
    "#finn_model = finn_model.transform(FoldConstants())\n",
    "#finn_model = finn_model.transform(GiveUniqueNodeNames())\n",
    "#finn_model = finn_model.transform(GiveReadableTensorNames())\n",
    "#finn_model = finn_model.transform(InferDataTypes())\n",
    "#finn_model = finn_model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "verif_model_filename = build_dir + \"finn-verification.onnx\"\n",
    "finn_model.save(verif_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Would the FINN compiler still work if we didn't do this?** The compilation step in the next notebook applies these transformations internally and would work fine, but we're going to use FINN's verification capabilities below and these require the tidy-up transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our ready-to-go model after the transformations. Note that all intermediate tensors now have their shapes specified (indicated by numbers next to the arrows going between layers). Additionally, the datatype inference step has propagated quantization annotations to the outputs of `MultiThreshold` layers (expand by clicking the + next to the name of the tensor to see the quantization annotation) and the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from finn.util.visualization import showInNetron\n",
    "#showInNetron(verif_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the Dataset and the Brevitas Model <a id=\"load_dataset\"></a>\n",
    "\n",
    "We'll use some example data from the quantized UNSW-NB15 dataset (from the previous notebook) to use as inputs for the verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def get_preqnt_dataset(data_dir: str, train: bool):\n",
    "    unsw_nb15_data = np.load(data_dir + \"/unsw_nb15_binarized.npz\")\n",
    "    if train:\n",
    "        partition = \"train\"\n",
    "    else:\n",
    "        partition = \"test\"\n",
    "    part_data = unsw_nb15_data[partition].astype(np.float32)\n",
    "    part_data = torch.from_numpy(part_data)\n",
    "    part_data_in = part_data[:, :-1]\n",
    "    part_data_out = part_data[:, -1]\n",
    "    return TensorDataset(part_data_in, part_data_out)\n",
    "\n",
    "n_verification_inputs = 100\n",
    "test_quantized_dataset = get_preqnt_dataset(\".\", False)\n",
    "input_tensor = test_quantized_dataset.tensors[0][:n_verification_inputs]\n",
    "input_tensor.shape\n",
    "\n",
    "train_quantized_dataset = get_preqnt_dataset(\".\", True)\n",
    "test_quantized_dataset = get_preqnt_dataset(\".\", False)\n",
    "\n",
    "print(\"Samples in each set: train = %d, test = %s\" % (len(train_quantized_dataset), len(test_quantized_dataset))) \n",
    "print(\"Shape of one input sample: \" +  str(train_quantized_dataset[0][0].shape))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "data_path = base_dir + \"CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"\n",
    "\n",
    "# Prepare dataset\n",
    "def load_dataset(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for subfolder in os.listdir(data_path):\n",
    "        subfolder_path = os.path.join(data_path, subfolder)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "        for image_filename in os.listdir(subfolder_path):\n",
    "            image_path = os.path.join(subfolder_path, image_filename)\n",
    "            images.append(image_path)\n",
    "            labels.append(subfolder)\n",
    "    return pd.DataFrame({'image': images, 'label': labels})\n",
    "\n",
    "# Define Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, class_indices=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.class_indices = class_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        #image = ImageOps.grayscale(Image.open(img_path)) \n",
    "        label = self.class_indices[self.dataframe.iloc[idx]['label']]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "input_size = (3, 224, 224)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size[1], input_size[2])),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "data = load_dataset(data_path)\n",
    "train_df, dummy_df = train_test_split(data, train_size=0.01, shuffle=True, stratify=data['label'], random_state=123)\n",
    "\n",
    "#class_indices = {label: idx for idx, label in enumerate(train_df['label'].unique())}\n",
    "\n",
    "class_indices = {'Normal': 0, 'Tumor': 1, 'Stone': 2, 'Cyst': 3}\n",
    "\n",
    "train_quantized_dataset = CustomDataset(train_df, transform=transform, class_indices=class_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also bring up the MLP we trained in Brevitas from the previous notebook. We'll compare its outputs to what is generated by FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_brevitas(current_inp):\n",
    "    brevitas_output = brevitas_model.forward(current_inp)\n",
    "    # apply sigmoid + threshold\n",
    "    # brevitas_output = torch.sigmoid(brevitas_output)\n",
    "    \n",
    "    #brevitas_output = (brevitas_output.detach().numpy() > 0.5) * 1\n",
    "    brevitas_output = brevitas_output.detach().numpy()\n",
    "    # convert output to bipolar\n",
    "    #brevitas_output = 2*brevitas_output - 1\n",
    "    return brevitas_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare FINN & Brevitas execution <a id=\"compare_brevitas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make helper functions to execute the same input with Brevitas and FINN. For FINN, we'll use the [`finn.core.onnx_exec`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#finn.core.onnx_exec.execute_onnx) function to execute the exported FINN-ONNX on the inputs. Note that this ONNX execution is for verification only; not for accelerated execution.\n",
    "\n",
    "Recall that the quantized values from the dataset are 593-bit binary {0, 1} vectors whereas our exported model takes 600-bit bipolar {-1, +1} vectors, so we'll have to preprocess it a bit before we can use it for verifying the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp):\n",
    "    #print(finn_model.graph.input)\n",
    "    finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "    finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "    print(\"model_in_shape: \", finnonnx_model_in_shape)\n",
    "    finnonnx_out_tensor_name = finn_model.graph.output[0].name\n",
    "    # convert input to numpy for FINN\n",
    "    current_inp = current_inp.detach().numpy()\n",
    "    #current_inp = current_inp.transpose(0, 2, 3, 1)\n",
    "    #current_inp = current_inp.astype(np.uint8)\n",
    "    print(\"current_inp shape: \", current_inp.shape)\n",
    "    # add padding and re-scale to bipolar\n",
    "    # current_inp = np.pad(current_inp, [(0, 0), (0, 7)])\n",
    "    # current_inp = 2*current_inp-1\n",
    "    # reshape to expected input (add 1 for batch dimension)\n",
    "    current_inp = current_inp.reshape(finnonnx_model_in_shape)\n",
    "    # create the input dictionary\n",
    "    input_dict = {finnonnx_in_tensor_name : current_inp} \n",
    "    # run with FINN's execute_onnx\n",
    "    output_dict = oxe.execute_onnx(finn_model, input_dict)\n",
    "    #get the output tensor\n",
    "    finn_output = output_dict[finnonnx_out_tensor_name] \n",
    "    return finn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call our inference helper functions for each input and compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FINN execution:   0%|                                   | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 0 nok 1:   0%|                                       | 0/124 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.5377603 -5.295051   4.5171604 -3.55958  ]]\n",
      "2\n",
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 0 nok 2:   0%|                                       | 0/124 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18.162422    1.2638035 -15.9425125 -17.792074 ]]\n",
      "0\n",
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 0 nok 3:   0%|                                       | 0/124 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.9452955  25.890787  -14.01224   -14.6839485]]\n",
      "1\n",
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 0 nok 4:   0%|                                       | 0/124 [00:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-13.211899   -2.9322095 -10.565343    9.203706 ]]\n",
      "3\n",
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 0 nok 5:   0%|                                       | 0/124 [00:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.7146232 -2.9200678 -3.7085078 -5.3141093]]\n",
      "0\n",
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 0 nok 6:   0%|                                       | 0/124 [00:13<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.481871  -6.13564  -12.020788  -5.518823]]\n",
      "0\n",
      "model_in_shape:  [1, 3, 224, 224]\n",
      "current_inp shape:  (1, 3, 224, 224)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m current_inp \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, input_size[\u001b[38;5;241m0\u001b[39m], input_size[\u001b[38;5;241m1\u001b[39m], input_size[\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#brevitas_output = inference_with_brevitas(current_inp)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m finn_output \u001b[38;5;241m=\u001b[39m \u001b[43minference_with_finn_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_inp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m brevitas_output \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#print(brevitas_output)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36minference_with_finn_onnx\u001b[0;34m(current_inp)\u001b[0m\n\u001b[1;32m     20\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {finnonnx_in_tensor_name : current_inp} \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# run with FINN's execute_onnx\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43moxe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#get the output tensor\u001b[39;00m\n\u001b[1;32m     24\u001b[0m finn_output \u001b[38;5;241m=\u001b[39m output_dict[finnonnx_out_tensor_name] \n",
      "File \u001b[0;32m/home/emanuel/workspace/finn/src/finn/core/onnx_exec.py:54\u001b[0m, in \u001b[0;36mexecute_onnx\u001b[0;34m(model, input_dict, return_full_exec_context, start_node, end_node)\u001b[0m\n\u001b[1;32m     52\u001b[0m model_exec_mode \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metadata_prop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (model_exec_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (model_exec_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecute_onnx_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_full_exec_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcheck_all_tensor_shapes_specified():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unspecified tensor shapes, try infer_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/emanuel/workspace/finn/deps/qonnx/src/qonnx/core/onnx_exec.py:178\u001b[0m, in \u001b[0;36mexecute_onnx\u001b[0;34m(model, input_dict, return_full_exec_context, start_node, end_node)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m subgraph:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_sanitize_quant_tensors() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;66;03m# round input values to match quantization annotation\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m         execution_context \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_quant_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     execute_node(node, execution_context, graph, return_full_exec_context, opset_version)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_sanitize_quant_tensors() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# round output values to quantization annotation\u001b[39;00m\n",
      "File \u001b[0;32m/home/emanuel/workspace/finn/deps/qonnx/src/qonnx/util/basic.py:291\u001b[0m, in \u001b[0;36msanitize_quant_values\u001b[0;34m(model, node_tensors, execution_context, check_values)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# TODO: vectorize with numpy\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnditer(current_values):\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    292\u001b[0m         has_to_be_rounded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/home/emanuel/workspace/finn/deps/qonnx/src/qonnx/core/datatype.py:200\u001b[0m, in \u001b[0;36mIntType.allowed\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallowed\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m value) \u001b[38;5;129;01mand\u001b[39;00m (value \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mis_integer()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "verify_range = trange(len(train_quantized_dataset), desc=\"FINN execution\", position=0, leave=True)\n",
    "#brevitas_model.eval()\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "\n",
    "\"\"\"\n",
    "for i in verify_range:\n",
    "    # run in Brevitas with PyTorch tensor\n",
    "    current_inp = input_tensor[i].reshape((1, 3, 224, 224))\n",
    "    brevitas_output = inference_with_brevitas(current_inp)\n",
    "    finn_output = inference_with_finn_onnx(current_inp)\n",
    "    # compare the outputs\n",
    "    ok += 1 if finn_output == brevitas_output else 0\n",
    "    nok += 1 if finn_output != brevitas_output else 0\n",
    "    verify_range.set_description(\"ok %d nok %d\" % (ok, nok))\n",
    "    verify_range.refresh()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for images, labels in train_quantized_dataset:\n",
    "    # run in Brevitas with PyTorch tensor\n",
    "    # print(images.shape)\n",
    "    current_inp = images.reshape((1, input_size[0], input_size[1], input_size[2]))\n",
    "    #brevitas_output = inference_with_brevitas(current_inp)\n",
    "    finn_output = inference_with_finn_onnx(current_inp)\n",
    "    brevitas_output = [0, 0, 0, 0]\n",
    "    #print(brevitas_output)\n",
    "    print(finn_output)\n",
    "    print(labels)\n",
    "    # compare the outputs\n",
    "    ok += 1 if (finn_output == brevitas_output).all() else 0\n",
    "    nok += 1 if (finn_output != brevitas_output).all() else 0\n",
    "    verify_range.set_description(\"ok %d nok %d\" % (ok, nok))\n",
    "    verify_range.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert ok == n_verification_inputs\n",
    "    print(\"Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\")\n",
    "except AssertionError:\n",
    "    assert False, \"Verification failed. Brevitas and FINN-ONNX execution outputs are NOT identical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our second notebook. In the next one, we'll take the ONNX model we just verified all the way down to FPGA hardware with the FINN compiler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
