{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e590bb98-5175-4395-b282-8b881d38bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install seaborn\n",
    "#!pip install tensorflow\n",
    "#!pip install brevitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e10a27b-7e2c-413a-a582-99e2f96ee6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.quant import Int8ActPerTensorFloat, Int8WeightPerTensorFloat, Int8Bias\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data path\n",
    "#data_path = \"~/workspace/finn/notebooks/FINN_Brevitas/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"\n",
    "data_path = \"/home/emanuel/workspace/finn/notebooks/FINN_Brevitas/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"\n",
    "\n",
    "# Prepare dataset\n",
    "def load_dataset(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for subfolder in os.listdir(data_path):\n",
    "        subfolder_path = os.path.join(data_path, subfolder)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "        for image_filename in os.listdir(subfolder_path):\n",
    "            image_path = os.path.join(subfolder_path, image_filename)\n",
    "            images.append(image_path)\n",
    "            labels.append(subfolder)\n",
    "    return pd.DataFrame({'image': images, 'label': labels})\n",
    "\n",
    "data = load_dataset(data_path)\n",
    "train_df, dummy_df = train_test_split(data, train_size=0.01, shuffle=True, stratify=data['label'], random_state=123)\n",
    "valid_df, dummy_df = train_test_split(dummy_df, train_size=0.01, shuffle=True, stratify=dummy_df['label'], random_state=123)\n",
    "test_df, dummy_df = train_test_split(dummy_df, train_size=0.01, shuffle=True, stratify=dummy_df['label'], random_state=123)\n",
    "\n",
    "# Define Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, class_indices=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.class_indices = class_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.class_indices[self.dataframe.iloc[idx]['label']]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0029cd-0df3-4991-a048-f74370676931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "\n",
    "input_size = (3, 224, 224)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size[1:3]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4355d3-fa58-4606-b742-ff46035af660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and loaders\n",
    "class_indices = {label: idx for idx, label in enumerate(train_df['label'].unique())}\n",
    "train_dataset = CustomDataset(train_df, transform=transform, class_indices=class_indices)\n",
    "valid_dataset = CustomDataset(valid_df, transform=transform, class_indices=class_indices)\n",
    "test_dataset = CustomDataset(test_df, transform=transform, class_indices=class_indices)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a79360-7ff7-4f34-b061-808670512861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantized MobileNetV2 model with better output layers\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "     \n",
    "\n",
    "class QuantMobileNetV2Model(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(QuantMobileNetV2Model, self).__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(3, 6, 5, bias=True, weight_bit_width=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = qnn.QuantConv2d(6, 16, 5, bias=True, weight_bit_width=4)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1   = qnn.QuantLinear(16*53*53, 120, bias=True, weight_bit_width=4)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2   = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=4)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3   = qnn.QuantLinear(84, num_classes, bias=True, weight_bit_width=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        out = self.relu1(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        #print(out.shape)\n",
    "        #out = out.reshape(out.shape[1]*out.shape[2]*out.shape[3])\n",
    "        out = out.view(-1, 16 * 53 * 53)\n",
    "        #print(out.shape)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.relu4(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class QuantMobileNetV2Model2(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(QuantMobileNetV2Model2, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(pretrained=True)\n",
    "        self.base_model.features[0][0] = qnn.QuantConv2d(3, 32, kernel_size=3, stride=2, padding=1, weight_bit_width=8, bias_quant=Int8Bias, weight_quant=Int8WeightPerTensorFloat)\n",
    "        self.base_model.features[0][1] = qnn.QuantReLU(quant_type=QuantType.INT, bit_width=8)\n",
    "        \n",
    "        # Replace all Conv2d and ReLU layers with quantized versions\n",
    "        for i, layer in enumerate(self.base_model.features):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.base_model.features[i] = qnn.QuantConv2d.from_float(layer, weight_bit_width=8, bias_quant=Int8Bias, weight_quant=Int8WeightPerTensorFloat)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                self.base_model.features[i] = qnn.QuantReLU(quant_type=QuantType.INT, bit_width=8)\n",
    "                \n",
    "        #self.base_model.classifier[1] = nn.Sequential(\n",
    "        #    nn.Linear(self.base_model.last_channel, 128),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Dropout(0.5),\n",
    "        #    nn.Linear(128, num_classes)\n",
    "        #)\n",
    "\n",
    "        self.base_model.classifier[1] = nn.Sequential(\n",
    "            qnn.QuantLinear(self.base_model.last_channel, 128, bias=True, weight_bit_width=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            qnn.QuantLinear(128, num_classes, bias=True, weight_bit_width=8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Instantiate model and set device\n",
    "quant_mobilenetv2_model = QuantMobileNetV2Model(num_classes=4).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "optimizer = optim.Adam(quant_mobilenetv2_model.parameters(), lr=0.001)\n",
    "criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa5b815-a038-47a8-a81c-7be2292f1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12])\n",
      "Epoch [1/20], Train Loss: 0.2926, Train Accuracy: 0.3871, Valid Loss: 0.1749, Valid Accuracy: 0.4065\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12])\n",
      "Epoch [2/20], Train Loss: 0.1713, Train Accuracy: 0.4113, Valid Loss: 0.1488, Valid Accuracy: 0.4309\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12])\n",
      "Epoch [3/20], Train Loss: 0.1466, Train Accuracy: 0.5242, Valid Loss: 0.1313, Valid Accuracy: 0.5203\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m train_losses, valid_losses, train_accuracies, valid_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_mobilenetv2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     14\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#print(images.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Wrap images in QuantTensor\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     62\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 63\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_indices[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            #print(images.shape)\n",
    "            # Wrap images in QuantTensor\n",
    "            quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(quant_images)\n",
    "            #print(outputs.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                # Wrap images in QuantTensor\n",
    "                quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "                outputs = model(quant_images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        valid_loss = running_loss / len(valid_loader)\n",
    "        valid_accuracy = correct / total\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_quant_mobilenetv2_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
    "\n",
    "num_epochs = 20\n",
    "# Train the model\n",
    "train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(quant_mobilenetv2_model, train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946537e7-c657-4ca5-846d-ce78e87e653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Wrap images in QuantTensor\n",
    "            quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "            outputs = model(quant_images)  # Use quantized images\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = evaluate_model(quant_mobilenetv2_model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb56b0-1e24-4cea-80bd-9da56df7fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "\n",
    "# Function to measure execution time\n",
    "#def measure_time(func, *args, **kwargs):\n",
    "    #start_time = time.time()  # Start time\n",
    "    #result = func(*args, **kwargs)  # Call the function\n",
    "    #end_time = time.time()  # End time\n",
    "    #execution_time = end_time - start_time  # Calculate execution time\n",
    "    #return result, execution_time  # Return result and execution time\n",
    "\n",
    "# Train the model and measure execution time\n",
    "#train_results, train_time = measure_time(train_model, \n",
    "                                          #quant_mobilenetv2_model, \n",
    "                                          #train_loader, \n",
    "                                          #valid_loader, \n",
    "                                          #criterion, \n",
    "                                          #optimizer, \n",
    "                                          #num_epochs=20, \n",
    "                                          #patience=5)\n",
    "\n",
    "#print(f'Training Time: {train_time:.2f} seconds')\n",
    "\n",
    "# Evaluate the model and measure execution time\n",
    "#test_results, test_time = measure_time(evaluate_model, quant_mobilenetv2_model, test_loader)\n",
    "\n",
    "#print(f'Evaluation Time: {test_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70546921-5816-47ee-bd11-5a5838e5fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Import time module for CPU timing\n",
    "\n",
    "def measure_inference_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    total_time = 0.0\n",
    "    total_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "\n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "            outputs = model(quant_images)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            total_time += elapsed_time\n",
    "            total_images += images.size(0)\n",
    "\n",
    "            # Memory usage\n",
    "            memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0  # MB\n",
    "            memory_reserved = torch.cuda.memory_reserved() / (1024 * 1024) if torch.cuda.is_available() else 0  # MB\n",
    "\n",
    "    avg_inference_time = (total_time / total_images) * 1000 if total_images > 0 else 0  # Convert to milliseconds\n",
    "    throughput = (total_images / total_time) if total_time > 0 else 0  # images per second\n",
    "    memory_usage = memory_allocated\n",
    "\n",
    "    return avg_inference_time, throughput, memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eabc34-4adb-4cc6-97d9-2e29446d685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Wrap images in QuantTensor\n",
    "            quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "            outputs = model(quant_images)  # Use quantized images\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Measure metrics during evaluation\n",
    "avg_inference_time, throughput, memory_usage = measure_inference_metrics(quant_mobilenetv2_model, test_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = evaluate_model(quant_mobilenetv2_model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Average Inference Time: {avg_inference_time:.4f} ms, Throughput: {throughput:.2f} images/s, Memory Usage: {memory_usage:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef10d84-252d-45c0-b583-31b126acd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_metrics(train_losses, valid_losses, train_accuracies, valid_accuracies, num_epochs):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, valid_losses, label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, valid_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(train_losses, valid_losses, train_accuracies, valid_accuracies, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f0942-2da4-4d1d-a06a-ddb9d230c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "quant_mobilenetv2_model.load_state_dict(torch.load('best_quant_mobilenetv2_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012caf3-aeb4-40be-99e6-89759dd41088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(model, data_loader, class_indices):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    # Create QuantTensor from images\n",
    "    quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "\n",
    "    outputs = model(quant_images)  # Use quantized images\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot images with predicted labels\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(8):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0).cpu())\n",
    "        plt.title(f'True: {list(class_indices.keys())[labels[i]]}\\nPred: {list(class_indices.keys())[predicted[i]]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f794a7-7dda-40df-83b9-3e1ad361dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display predictions\n",
    "display_predictions(quant_mobilenetv2_model, test_loader, class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8851d03-d8a8-438b-8049-6c6348c43bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Wrap images in QuantTensor\n",
    "            quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "            outputs = model(quant_images)  # Use quantized images\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect true and predicted labels\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "    \n",
    "    return y_true, y_pred, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de052e06-eaf9-4f05-bbbd-2c0f318e7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and get true and predicted labels\n",
    "y_true, y_pred, test_loss, test_accuracy = evaluate_model(quant_mobilenetv2_model, test_loader)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=list(class_indices.keys()), columns=list(class_indices.keys()))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406dc15-ddcc-4bdd-88c0-07425f7d7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from brevitas.quant_tensor import QuantTensor  # Ensure you have this import\n",
    "\n",
    "# Assuming device is already defined in your previous code\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to preprocess and predict on a single image\n",
    "def predict_image(image_path, model, transform, class_indices):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Preprocess the image and move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Wrap image in QuantTensor if using quantized model\n",
    "        quant_image = QuantTensor(image, scale=1.0, bit_width=8)  # Only if using quantized model\n",
    "        output = model(quant_image)  # Pass quantized image to the model\n",
    "        probabilities = F.softmax(output, dim=1).cpu().numpy()[0]  # Get probabilities\n",
    "        predicted_class = np.argmax(probabilities)  # Get predicted class index\n",
    "        predicted_label = list(class_indices.keys())[predicted_class]  # Get predicted label\n",
    "\n",
    "    return predicted_label, probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d2186-ae40-4786-b2c5-48b8236b0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display the prediction results\n",
    "def display_prediction(image_path, model, transform, class_indices):\n",
    "    predicted_label, probabilities = predict_image(image_path, model, transform, class_indices)\n",
    "    \n",
    "    print(f'Predicted Label: {predicted_label}')\n",
    "    for label, probability in zip(class_indices.keys(), probabilities):\n",
    "        print(f'{label}: {probability:.2%}')  # Display probabilities as percentages\n",
    "\n",
    "    # Display the image\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Prediction: {predicted_label}')\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Upload and predict an image\n",
    "# uploaded_image_path = '/home/vitis/finn/notebooks/pic1.jpg'  # Replace this with the actual image path\n",
    "\n",
    "# Assuming 'mobilenetv2_model', 'transform', and 'class_indices' are already defined\n",
    "#display_prediction(uploaded_image_path, quant_mobilenetv2_model, transform, class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a6ff4-00c9-43e7-9c7e-b789768d5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploaded_image_path = '/home/vitis/finn/notebooks/pic2s.jpg'  # Replace this with the actual image path\n",
    "#display_prediction(uploaded_image_path, quant_mobilenetv2_model, transform, class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b95c45e-cc49-4bb2-8130-36f7e0c1f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploaded_image_path = '/home/vitis/finn/notebooks/pic3n.jpg'  # Replace this with the actual image path\n",
    "#display_prediction(uploaded_image_path, quant_mobilenetv2_model, transform, class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59533b32-92b4-4e00-935a-4f191d1b7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploaded_image_path = '/home/vitis/finn/notebooks/pic4c.jpg'  # Replace this with the actual image path\n",
    "#display_prediction(uploaded_image_path, quant_mobilenetv2_model, transform, class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5eb44-1b1c-4be0-8246-3ca779f57c9f",
   "metadata": {},
   "source": [
    "# Step 1: Export the Quantized Model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f6475-f3fc-4dbb-bd60-a22f102a3258",
   "metadata": {},
   "source": [
    "After evaluating the quantized model, we'll export it to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd89f18-f48b-46c5-97fc-803909c08455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "\n",
    "# input is a QuantTensor, so create a random one\n",
    "# random image, a float from 0 to 1.0\n",
    "input_a = np.random.randint(0, 1.0, size=input_shape).astype(np.float32)\n",
    "input_b = torch.from_numpy(input_a)\n",
    "\n",
    "#input_b = (input_b / input_scale + input_zero_point).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "#input_t = QuantTensor(input_b, scale=1.0, zero_point=0.0, bit_width=8, signed=True, training=True)\n",
    "input_t = QuantTensor(input_b, scale=1.0, zero_point=0.0, bit_width=8, signed=False, training=True)\n",
    "\n",
    "#Move to CPU before export\n",
    "quant_mobilenetv2_model.cpu()\n",
    "\n",
    "finn_model_filename = \"finn.onnx\"\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(quant_mobilenetv2_model, export_path=finn_model_filename, input_t=input_t)\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(finn_model_filename, out_file=finn_model_filename)\n",
    "\n",
    "# ModelWrapper\n",
    "finn_model = ModelWrapper(finn_model_filename)\n",
    "finn_model = finn_model.transform(ConvertQONNXtoFINN())\n",
    "finn_model.save(finn_model_filename)\n",
    "\n",
    "print(\"Model saved to %s\" % finn_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd8d57-485e-4250-b944-75e525d0c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from finn.util.visualization import showInNetron\n",
    "\n",
    "#showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115593ea-59b9-40ea-811c-fa7038f1f848",
   "metadata": {},
   "source": [
    "# Step 2: Test the FINN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8950b-ee62-4e97-ae31-84e723e153ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_out_tensor_name = finn_model.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = finn_model.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = finn_model.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = finn_model.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in finn_model.graph.node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb6660-5447-4192-842e-16043729eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "finn_model = finn_model.transform(InferShapes())\n",
    "finn_model = finn_model.transform(FoldConstants())\n",
    "finn_model = finn_model.transform(GiveUniqueNodeNames())\n",
    "finn_model = finn_model.transform(GiveReadableTensorNames())\n",
    "finn_model = finn_model.transform(InferDataTypes())\n",
    "finn_model = finn_model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "finn_verif_model_filename = \"fin_verif.onnx\"\n",
    "finn_model.save(finn_verif_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0170e99-48cc-4232-ab68-0d8ca811c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp):\n",
    "    finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "    finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "    finnonnx_out_tensor_name = finn_model.graph.output[0].name\n",
    "    # convert input to numpy for FINN\n",
    "    current_inp = current_inp.detach().numpy()\n",
    "    # reshape to expected input (add 1 for batch dimension)\n",
    "    #current_inp = current_inp.reshape(finnonnx_model_in_shape)\n",
    "    #print(current_inp.shape)\n",
    "    # create the input dictionary\n",
    "    input_dict = {finnonnx_in_tensor_name : current_inp} \n",
    "    # run with FINN's execute_onnx\n",
    "    output_dict = oxe.execute_onnx(finn_model, input_dict)\n",
    "    #get the output tensor\n",
    "    finn_output = output_dict[finnonnx_out_tensor_name] \n",
    "    return finn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07f430-4819-4ad9-9917-2bc1752620c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_finn_predictions(data_loader, class_indices):\n",
    "    #model.eval()\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    outputs = inference_with_finn_onnx(images[0:1])\n",
    "    outputs = torch.tensor(outputs)\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot images with predicted labels\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.imshow(images[0].permute(1, 2, 0).cpu())\n",
    "    plt.title(f'True: {list(class_indices.keys())[labels[0]]}\\nPred: {list(class_indices.keys())[predicted[0]]}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_finn_predictions(test_loader, class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0398d8-8522-4c57-b8c9-3dd2d2b72faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_finn_model(test_loader):\n",
    "    #model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if images.shape[0] != batch_size:\n",
    "                continue\n",
    "            # Wrap images in QuantTensor\n",
    "            #quant_images = QuantTensor(images, scale=1.0, bit_width=8)\n",
    "            #outputs = model(quant_images)  # Use quantized images\n",
    "            outputs = inference_with_finn_onnx(images)\n",
    "            outputs = torch.tensor(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Measure metrics during evaluation\n",
    "#avg_inference_time, throughput, memory_usage = measure_inference_metrics(quant_mobilenetv2_model, test_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = evaluate_finn_model(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "#print(f'Average Inference Time: {avg_inference_time:.4f} ms, Throughput: {throughput:.2f} images/s, Memory Usage: {memory_usage:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06407535-6b28-48b2-98f7-e11c0302b586",
   "metadata": {},
   "source": [
    "## Launch a Build: Only Estimate Reports <a id=\"build_estimate_report\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee41b4-180f-45b5-a60f-00149056a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "estimates_output_dir = \"output_estimates_only\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165f15e-9b34-4154-95d2-516d33882c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(finn_model_filename, cfg_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164000a-f864-4e81-8004-8da5054e8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(estimates_output_dir + \"/report/estimate_network_performance.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5149e-a104-43da-b2ad-c0938a88eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {estimates_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590284f-8897-4ffc-973f-ab3d70c186f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {estimates_output_dir}/report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33384d-cb93-4663-ae4d-4b442def3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {estimates_output_dir}/report/estimate_network_performance.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3dff3-40fc-4888-b89a-22ffc7c378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json_dict(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ret = json.load(f)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cdf37-327c-4f36-821e-1b622502bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_cycles.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0ad9d-0ef5-422d-b989-da41e6484375",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_resources.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f55177-0908-4217-8e67-26ecd2a28cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "rtlsim_output_dir = \"output_ipstitch_ooc_rtlsim\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(rtlsim_output_dir):\n",
    "    shutil.rmtree(rtlsim_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "cfg_stitched_ip = build.DataflowBuildConfig(\n",
    "    output_dir          = rtlsim_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65231140-cea8-4e98-bff0-2795c820bebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48267ac-b496-4913-a3b3-6dc2025134ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(finn_model_filename, cfg_stitched_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0ec59-16c2-4668-97c5-631fd80fe188",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(rtlsim_output_dir + \"/report/ooc_synth_and_timing.json\")\n",
    "assert os.path.exists(rtlsim_output_dir + \"/report/rtlsim_performance.json\")\n",
    "assert os.path.exists(rtlsim_output_dir + \"/final_hw_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0119b-5fcc-4935-b057-50050583f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {rtlsim_output_dir}/stitched_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8689e13-294e-44be-aae8-6e4002f128a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {rtlsim_output_dir}/report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fe442-ea26-4ef5-b4c2-e673add67792",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {rtlsim_output_dir}/report/ooc_synth_and_timing.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ae7b4-3eea-4537-81aa-cc137c22fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {rtlsim_output_dir}/report/rtlsim_performance.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac63ba-71b9-48e0-b3d0-c9991ff0f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {rtlsim_output_dir}/final_hw_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023394de-3b69-4601-9bba-33bdf77d8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = model_dir + \"/cybsec-mlp-ready.onnx\"\n",
    "\n",
    "final_output_dir = \"output_final\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(final_output_dir):\n",
    "    shutil.rmtree(final_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "cfg = build.DataflowBuildConfig(\n",
    "    output_dir          = final_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    board               = \"Pynq-Z1\",\n",
    "    shell_flow_type     = build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.PYNQ_DRIVER,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
